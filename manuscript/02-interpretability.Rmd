```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
```

# Interpretability {#interpretability}

Throughout the book, I will use this rather simple, yet elegant definition of interpretability from  Miller (2017) [^Miller2017] : **Interpretability is the degree to which a human can understand the cause of a decision.**
Another take is: **Interpretability is the degree to which a human can consistently predict the model's result**.
The higher the interpretability of a model, the easier it is for someone to comprehend why certain decisions (read: predictions) were made.
A model has better interpretability than another model, if its decisions are easier to comprehend for a human than decisions from the second model.
I will be using both the terms interpretable and explainable equally.
Like  Miller (2017), I believe it makes sense to distinguish between the terms interpretability/explainability and explanation.
Making a machine learning interpretable can, but does not necessarily have to, imply providing a (human-style) explanation of a prediction.
See the [section about explanations](#explanation) to learn what we humans see as a good explanation.

## The Importance of Interpretability {#interpretability-importance}
If a machine learning model performs well, **why not just trust the model** and ignore **why** it made a certain decision?
"The problem is that a single metric, such as classification accuracy, is an incomplete description of most real-world tasks." (Doshi-Velez and Kim 2017 [^Doshi2017])

Let's dive deeper into the reasons why interpretability is so important.
In predictive modelling, you have to make a trade-off:
Do you simply want to know **what** is predicted?
For example the probability that a client will churn or how effective some medication will be for a patient.
Or do you want to know **why** the prediction was made and possibly paying for the interpretability with a drop in accuracy?
In some cases you don't care why a decision was made, only the assurance that the predictive performance was good on a test dataset is enough.
But in other cases, knowing the 'why' can help you understand more about the problem, the data and why a model might fail.
Some models might not need explanations, because they are used in a low risk environment, meaning a mistake has no severe consequences, (e.g. a movie recommender system) or the method has already been extensively studied and evaluated (e.g. optical character recognition).
The necessity for interpretability comes from an incompleteness in the problem formalisation (Doshi-Velez and Kim 2017), meaning that for certain problems or tasks it is not enough to get the answer (the **what**).
The model also has to give an explanation how it came to the answer (the **why**), because a correct prediction only partially solves your original problem.
The following reasons drive the demand for interpretability and explanations (Doshi-Velez and Kim 2017 and Miller 2017)
**Human curiosity and learning**: Humans have a mental model of their environment, which gets updated when something unexpected happens.
This update is done by finding an explanation for the unexpected event.
For example, a human feels unexpectedly sick and asks himself: "Why do I feel so sick?".
He learns that he becomes sick every time he eats those red berries.
He updates his mental model and decides that the berries caused the sickness and therefore should be avoided.
Curiosity and learning is important for any machine learning model used in the research context, where scientific findings stay completely hidden, when the machine learning model only gives predictions without explanations.
To facilitate learning and satisfy curiosity about why certain predictions or behaviours are created by machines, interpretability and explanations are crucial.
Of course, humans don't need an explanation for everything that happens.
Most people are okay with not understanding how a computer works.
The emphasis of this point is more on unexpected events, that makes us curious.
Like: Why is my computer shutting down unexpectedly?


Closely related to learning is the human desire to **find meaning in the world**.
We want to reconcile contradictions or inconsistencies between elements of our knowledge structures.
"Why did my dog bite me, even though it has never done so before?" a human might ask himself.
There is a contraction between the knowledge about the dog's past behaviour and the newly made, unpleasant experience of the bite.
The explanation of the vet reconciles the dog holders contradiction:
"The dog was under stress and did bite, dogs are animals and this can happen."
The more a machine's decision affects a human's life, the more important it will be for the machine to explain its behaviour.
When a machine learning model rejects a loan application, this could be quite unexpected for the applicant.
He can only reconcile this inconsistency between expectation and reality by having some form of explanation.
The explanations don't actually have to fully explain the situation, but should address a main cause.
Another example is algorithmic product recommendation.
Personally, I always reflect on why certain products or movies have been recommended to me algorithmically.
Often it is quite clear:
The advertising is following me on the Internet because I have bought a washing machine recently, and I know that I will be followed by washing machine advertisements the next days.
Yes, it makes sense to suggest gloves, when I already have a winter hat in my shopping basket.
The algorithm recommended this movie, because users that liked other movies that I also liked, enjoyed the recommended movie.
Increasingly, Internet companies are adding explanations to their recommendations.
A good example is the Amazon product recommendation based on frequently bought product combinations:
```{r amazon-recommendation, fig.cap='Recommended products when buying some paint from [Amazon](https://www.amazon.com/Colore-Acrylic-Paint-Set-12/dp/B014UMGA5W/). Visited on December 5th 2012.'}
knitr::include_graphics("images/amazon-freq-bought-together.png")
```


There is a shift in many scientific disciplines from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics).
The **goal of science** is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models.
The model itself becomes a source of knowledge, instead of the data.
Interpretability allows to tap into this additional knowledge captured by the model.

Machine learning models are taking over real world tasks, that demand **safety measures** and testing.
Imagine a self-driving car automatically detects cyclists, which is as desired.
You want to be 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad.
An explanation might reveal that the most important feature learned is to recognise the two wheels of a bike and this explanation helps you to think about edge cases like bikes with side bags, that partially cover the wheels.

By default most machine learning models pick up biases from the training data.
This can turn your machine learning models into racists which discriminate against protected groups.
Interpretability is a useful debugging tool to **detect bias** in machine learning models.
It might happen that the machine learning model you trained for automatically approving or rejecting loan applications discriminates against some minority.
Your main goal is to give out loans to people that will pay them back eventually.
In this case, the incompleteness in the problem formulation lies in the fact that you not only want to minimise loan defaults, but you are also required to not discriminate based on certain demographics.
This is an additional constraint, which is part of your problem formulation (Handing out loans in a low-risk and compliant way), which is not captured by the loss function, which the machine learning model optimises.

The process of integrating machines and algorithms into our daily lives demands interpretability to increase **social acceptance**.
People attribute beliefs, desires, intentions and so on to objects.
In a famous experiment, Heider and Simmel (1944) [^Heider] showed the participants videos of shapes, where a circle opened a door to enter a "room" (which was simply a rectangle).
The participants described the actions of the shapes as they would describe the actions of a human agent, attributing intentions and even emotions and personality traits to the shapes.
Robots are a good example, like my vacuum cleaner, which I named 'Doge'.
When Doge gets stuck, I think:
"Doge wants to continue cleaning, but asks me for help because it got stuck."
Later, when Doge finished cleaning and searches the home base to recharge I think:
"Doge has the desire to recharge and intents to find the home base".
Also I attribute personality traits:
"Doge is a bit dumb, but in a cute way".
These are my thoughts, especially when I find out that Doge threw over some plant while cleaning the house dutifully.
A machine or algorithm explaining its prediction will receive more acceptance.
See also the [chapter about explanations](#explanation), which argues that explanations are a social process.


Explanations are used to **manage social interactions**.
Through the creation of a shared meaning of something, the explainer influences the actions, emotions and beliefs of the receiver of the explanation.
In order to allow a machine to interact with us, it might need to shape our emotions and beliefs.
Machines have to "persuade" us, so that we believe that they can achieve their intended goal.
I would not completely accept my robot vacuum cleaner if it would not explain its behaviour to some degree.
The vacuum cleaner creates a shared meaning of, for example, an "accident" (like getting stuck on the bathroom carpet ... again) by explaining that it got stuck, instead of simply stopping to work without comment.
Interestingly, there can be a misalignment between the goal of the explaining machine, which is generating trust, and the goal of the recipient, which is to understand the prediction or behaviour.
Maybe the correct explanation why Doge got stuck could be that the battery was very low, additionally one of the wheels is not working properly and on top of that there is a bug that causes the robot to re-try to go to the same spot over and over again, even though there was some obstacle in the way.
These reasons (and some more) caused the robot to get stuck, but it only explained that there was something in the way, and this was enough for me to trust its behaviour, and to get a shared meaning of that accident, which I can share with my girlfriend.
("By the way, Doge got stuck again in the bathroom, we have to remove the carpets before we let it clean").
The example of the robot getting stuck on the carpet might not even require an explanation, because I can explain it to myself by observing that Doge can't move on this carpet mess.
But there are other situations, which are less obvious, like a full dirt bag.

```{r doge-stuck, fig.cap="Doge, my vacuum cleaner got stuck. As an explanation for the accident, Doge told me that it needs to be on a flat surface."}
knitr::include_graphics("images/doge-stuck.png")
```

Only with interpretability can machine learning algorithms be **debugged and audited**.
So even in low risk environments, like movie recommendation, interpretability in the research and development stage as well as after deployment is valuable.
Because later, when some model is used in a product, things can go wrong.
Having an interpretation for a faulty prediction helps to understand the cause of the fault.
It delivers a direction for how to fix the system.
Consider an example of a husky versus wolf classifier, that misclassifies some huskies as wolfs.
Using interpretable machine learning methods, you would find out that the misclassification happened due to the snow on the image.
The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training dataset, but not in the real world use.

If you can ensure that the machine learning model can explain decisions, the following traits can also be checked more easily (Doshi-Velez and Kim 2017):

- Fairness: Making sure the predictions are unbiased and not discriminating against protected groups (implicit or explicit).
An interpretable model can tell you why it decided that a certain person is not worthy of a credit and for a human it becomes easier to judge if the decision was based on a learned demographic (e.g. racial) bias.
- Privacy: Ensuring that sensitive information in the data is protected.
- Reliability or Robustness: Test that small changes in the input don't lead to big changes in the prediction.
- Causality: Check if only causal relationships are picked up. Meaning a predicted change in a decision due to arbitrary changes in the input values are also happening in reality.
- Trust: It is easier for humans to trust a system that explains its decisions compared to a black box.

**When we don't need interpretability.**

The following scenarios illustrate when we don't need or even don't want interpretability for machine learning models.

Interpretability is not required if the model **has no significant impact**.
Imagine someone named Mike working on a machine learning side project to predict where his friends will go to for their next vacation based on Facebook data.
Mike just likes it to surprise his friends with educated guesses where they're going on vacation.
There is no real problem if the model is wrong (just a little embarrassment for Mike), it is also not problematic if Mike can't explain the output of his model.
It's perfectly fine not to have any interpretability.
The situation would change when Mike starts building a company around these vacation destination predictions.
If the model is wrong, the company will lose money, or the model could refuse services to people based on some learned racial bias.
As soon as the model has a significant impact, either financially or socially, the interpretability becomes relevant.

Interpretability is not required when the **problem is well-studied**.
Some applications have been sufficiently well-studied so that there is enough practical experience with the model and problems with the model have been solved over time.
A good example is a machine learning model for optical character recognition that processes images of envelopes and extracts the addresses.
There are years of experience in using these systems and it is clear that they work. 
Also we are not really interested in gaining additional insights about the task at hand. 

Interpretability might enable **gaming the system**.
Problems with users fooling a system result from a mismatch in the objectives of the creator and the user of a model.
Credit scoring is one such system because the banks want to ensure to give loans to applicants who are likely to return it and the applicants have the goal to get the loan even if the bank has a different opinion.
This mismatch between objectives introduces incentives for the applicants to game the system to increase their chances of getting a loan.
If an applicant knows that having more than two credit cards affects the score negatively, he simply returns his third credit card to improve his score, and simply gets a new card after the loan has been approved.
While his score improved, the true probability of repaying the loan remained the same.
The system can only be gamed if the inputs are proxies for another feature, but are not the cause of the outcome.
Whenever possible, proxy features should be avoided, as they are often the reason for defective models. 
For example, Google developed a system called Google Flu Trends for predicting flu outbreaks that correlates Google searches with flu outbreaks - and it performed rather poorly so far. 
The distribution of searches changed and Google Flu Trends missed many flu outbreaks.
Google searches are not known to cause the flu and people searching for symptoms like "fever" merely imply a correlation.
Ideally, models would only use causal features, because then they are not gameable and a lot of issues with biases would not occur.

## Criteria for Interpretability Methods

Methods for machine learning interpretability can be classified according to different criteria:

- **Intrinsic or post hoc?**
Intrinsic interpretability means selecting and training a machine learning model that is considered to be intrinsically interpretable (for example short decision trees). 
Post hoc interpretability means selecting and training a black box model (for example a neural network) and applying interpretability methods after the training (for example measuring the feature importance).
The "intrinsic or post hoc"-criterion determined the layout of the chapters in the book:
The two main chapters are the [intrinsically interpretable models chapter](#simple) and the [post hoc (and model-agnostic) interpretability methods chapter](#agnostic).
- **Outcome of the interpretability method**:
The different interpretability methods can be roughly differentiated according to their outcomes:
    - **Feature summary statistic**: Many interpretability methods provide a kind of summary statistic of how each feature affects the model predictions. These can be feature importance measures or statistics about the interaction strength between features.
    - **Feature summary visualization**: Most feature summary statistics can be visualized. However, some feature summaries can only be visualized and not meaningfully be placed in a table. The partial dependence of a feature is such a case. For non-linear relationships, partial dependence plots are arbitrary curves showing a feature and the average predicted outcome.
    - **Model internals (e.g. learned weights)**: The interpretation of intrinsically interpretable models falls under this category. Examples are the weights in linear models or the learned tree structure (which features and feature values are used for the splits?) of decision trees. The lines are blurred between model internals and feature summary statistic in, for example, linear models, because the weights are both model internals and at the same time summary statistics for the features. Another method that outputs model internals is the visualization of feature detectors that are learned in convolutional neural networks. Interpretability methods that output model internals are model-specific by definition (see next point). 
    - **Data point**: This category includes all methods that return data points (can be existing or newly created) to make a model interpretable. Counterfactuals, for example: To explain the prediction of a data point, find a similar data point by changing some of the features for which the predicted outcome changes in a relevant way (like a flip in the predicted class). Another example is the identification of prototypes of predicted classes. Interpretability methods that output new data points require that the data points themselves can be interpreted. This works well for images and text, but is less useful for tabular data with hundreds of features.
    - **Intrinsically interpretable model**: This is a little circular, but one solution to interpreting black box models is to approximate them (either globally or locally) with an interpretable model. The interpretable model themselves are interpreted by internal model parameter or feature summary statistics.
- **Model-specific or model-agnostic?**:
Model-specific interpretation tools are limited to specific model classes.
The interpretation of regression weights in a linear model is a model-specific interpretation, since - by definition - the interpretation of intrinsically interpretable models is always model-specific.
Any tool that only works for e.g. interpreting neural networks is model-specific.
Model-agnostic tools can be used on any machine learning model and are usually post hoc.
These agnostic methods usually operate by analysing feature input and output pairs. 
By definition, these methods can't have access to any model internals like weights or structural information.
- **Local or global?**:
Does the interpretation method explain a single prediction or the entire model behavior? Or is the scope somewhere in between?
Read more about the scope criterion in the next section.


## Scope of Interpretability
An algorithm trains a model, which produces the predictions. Each step can be evaluated in terms of transparency or interpretability.

###  Algorithm transparency
*How does the algorithm create the model?*

Algorithm transparency is about how the algorithm learns a model from the data and what kind of relationships it is capable of picking up.
If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers.
This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single predictions are made.
For this level of transparency, only knowledge about the algorithm and not about the data or concrete learned models are required.
This book focuses on model interpretability and not algorithm transparency.
Algorithms like the least squares method for linear models are well studied and understood.
They score high in transparency.
Deep learning approaches (pushing a gradient through a network with millions of weights) are less well understood and the inner workings are in the focus of on-going research.
It is not clear how they exactly work, so they are less transparent.


### Global, Holistic Model Interpretability
*How does the trained model make predictions?*

You could call a model interpretable if you can comprehend the whole model at once (Lipton 2016[^Lipton2016]).
To explain the global model output, you need the trained model, knowledge about the algorithm and the data.
This level of interpretability is about understanding how the model makes the decisions, based on a holistic view of its features and each of the learned components like weights, parameters, and structures.
Which features are the important ones and what kind of interactions are happening?
Global model interpretability helps to understand the distribution of your target variable based on the features.
Arguably, global model interpretability is very hard to achieve in practice.
Any model that exceeds a handful of parameters or weights, probably won't fit in an average human's short term memory.
I'd argue that you cannot really imagine a linear model with 5 features and draw in your head the hyperplane that was estimated in the 5-dimensional feature space.
Each feature space with more than 3 dimensions is just not imaginable for humans.
Usually when people try to comprehend a model, they look at parts of it, like the weights in linear models.

### Global Model Interpretability on a Modular Level
*How do parts of the model influence predictions?*


You might not be able to comprehend a Naive Bayes model with many hundred features, because there is no way you could
hold all the feature weights in your brain's working memory.
But you can understand a single weight easily.
Not many models are interpretable on a strict parameter level.
While global model interpretability is usually out of reach, there is a better chance to understand at least some models on a modular level.
In the case of linear models, the interpretable parts are the weights and the distribution of the features, for trees it would be splits (used feature plus the cut-off point) and leaf node predictions.
Linear models for example look like they would be perfectly interpretable on a modular level, but the interpretation of a single weight is interlocked with all of the other weights.
The interpretation of a single weight always comes with the footnote that the other input features stay at the same value, which is not the case in many real world applications.
A linear model predicting the value of a house, which takes into account both the size of the house and the number of rooms might have a negative weight for the rooms feature, which is counter intuitive.
But it can happen, because there is already the highly correlated flat size feature and in a market where people prefer bigger rooms, a flat with less rooms might be worth more than a flat with more rooms when both have the same size. The weights only make sense in the context of the other features used in the model.
But arguably the weights in a linear model still have better interpretability than the weights of a deep neural network.


### Local Interpretability for a Single Prediction
*Why did the model make a specific decision for an instance?*

You can zoom in on a single instance and examine what kind of prediction the model makes for this input, and why it made this decision.
When you look at one example, the local distribution of the target variable might behave more nicely.
Locally it might depend only linearly or monotonic on some features rather than having a complex dependence on the features.
For example the value of an apartment might not depend linearly on the size.
But if you only look at a specific apartment of 100 square meters and check how the price changes by going up and down by 10 square meters, there is a chance that this subregion in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.
This book presents methods that can make single predictions more interpretable in the [section about model-agnostic methods](#agnostic).

### Local Interpretability for a Group of Predictions
*Why did the model make specific decisions for a group of instances?*

The model predictions for multiple instances can be explained by either using methods for global model interpretability (on a modular level) or single instance explanations.
The global methods can be applied by taking the group of instances, pretending it's the complete dataset, and using the global methods on this subset.
The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group.

## Evaluating Interpretability
There is no real consensus on what interpretability in machine learning is.
Also it is not clear how to measure it.
But there is some first research on it and the attempt to formulate some approaches for the evaluation, as described in the following section.

Doshi-Velez and Kim (2017) propose three major levels when evaluating interpretability:

- **Application level evaluation (real task)**: Put the explanation into the product and let the end user test it.
For example, on an application level, radiologists would test fracture detection software (which includes a machine learning component to suggest where fractures might be in an x-ray image) directly in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality.
A good baseline for this is always how good a human would be at explaining the same decision.
- **Human level evaluation (simple task)** is a  simplified application level evaluation.
The difference is that these experiments are not conducted with the domain experts, but with lay humans.
This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans.
An example would be to show a user different explanations and the human would choose the best.
- **Function level evaluation (proxy task)** does not require any humans.
This works best when the class of models used was already evaluated by someone else in a human level evaluation. For example it might be known that the end users understand decision trees.
In this case, a proxy for explanation quality might be the depth of the tree.
Shorter trees would get a better explainability rating.
It would make sense to add the constraint that the predictive performance of the tree remains good and does not drop too much compared to a larger tree.


**More on Function Level Evaluation**

Model size is an easy way to measure explanation quality, but it is too simplistic.
For example, a sparse model with features that are themselves not interpretable is still not a good explanation.

There are more dimensions to interpretability:

- Model sparsity: How many features are being used by the explanation?
- Monotonicity: Is there a monotonicity constraint?
Monotonicity means that a feature has a monotonic relationship with the target.
If the feature increases, the target either always increases or always decreases, but never switches between increasing and decreasing.
- Uncertainty: Is a measurement of uncertainty part of the explanation?
- Interactions: Is the explanation able to include interactions of features?
- Cognitive processing time: How long does it take to understand the explanation?
- Feature complexity: What features were used for the explanation? PCA components are harder to understand than word occurrences, for example.
- Description length of explanation.



## Human-friendly Explanations {#explanation}

Let's dig deeper and discover what we humans accept as 'good' explanations and what the implications for interpretable machine learning are.

Research from the humanities can help us to figure that out.
Miller (2017) did a huge survey of publications about explanations and this Chapter builds on his summary.

In this Chapter, I want to convince you of the following:
As an explanation for an event, humans prefer short explanations (just 1 or 2 causes), which contrast the current situation with a situation where the event would not have happened.
Especially abnormal causes make good explanations.
Explanations are social interactions between the explainer and the explainee (receiver of the explanation) and therefore the social context has a huge influence on the actual content of the explanation.

If you build the explanation system to get ALL the factors for a certain prediction or behaviour, you do not want a human-style explanation, but rather a complete causal attribution.
You probably want a causal attribution when you are legally required to state all influencing features or if you are debugging the machine learning model.
In this case, ignore the following points.
In all other setting, where mostly lay persons or people with little time are the recipients of the explanation, follow the advice here.


### What is an explanation?

An explanation is the **answer to a why-question** (Miller 2017).

- Why did the treatment not work on the patient?
- Why was my loan rejected?
- Why haven't we been contacted by alien life yet?

The first two kind of questions can be answered with an "everyday"-explanation, while the third one is from the category "More general scientific phenomena and philosophical questions".
We focus on the "everyday"-type explanation, because this is relevant for interpretable machine learning.
Questions starting with "how" can usually be turned into "why" questions:
"How was my loan rejected?" can be turned into "Why was my loan rejected".


The term "explanation" means the social and cognitive process of explaining, but it's also the product of these processes.
The explainer can be a human or a machine


### What is a "good" explanation? {#good-explanation}

Now that we know what an explanation is, the question arises, what a good explanation is.

"Many artificial intelligence and machine learning publications and methods claim to be about 'interpretable' or 'explainable' methods, yet often this claim is only based on the authors intuition instead of hard facts and research." - Miller (2017)

Miller (2017) summarises what a 'good' explanation is, which this Chapter replicates in condensed form and with concrete suggestions for machine learning applications.

**Explanations are contrastive** (Lipton 2016):
Humans usually don't ask why a certain prediction was made, but rather why this prediction was made instead of another prediction.
We tend to think in counterfactual cases, i.e. "How would the prediction have looked like, if input X were different?".
For a house value prediction, a person might be interested in why the predicted price was high compared to the lower price she expected.
When my loan application is rejected, I am not interested what in general constitutes a rejection or an approval.
I am interested in the factors of my application that would need to change so that it got accepted.
I want to know the contrast between my application and the would-be-accepted version of my application.
The realisation that contrastive explanations matter, is an important finding for explainable machine learning.
As we will see, most interpretable models allow to extract some form of explanation that implicitly contrast it to an artificial data instance or an average of instances.
A doctor who wonders: "Why did the treatment not work on the patient?", might ask for an explanation contrastive to a patient, where the treatment worked and who is similar to the non-responsive patient.
Contrastive explanations are easier to understand than complete explanations.
A complete explanation to the doctor's why question (why does the treatment not work) might include:
The patient has the disease already since 10 years, 11 genes are over-expressed making the disease more severe, the patients body is very fast in breaking down the medication into ineffective chemicals , etc..
The contrastive explanation, which answers the question compared to the other patient, for whom the drug worked, might be much simpler:
The non-responsive patient has a certain combination of genes, that make the medication much less effective, compared to the other patient.
The best explanation is the one that highlights the greatest difference between the object of interest and the reference object.
**What it means for interpretable machine learning**:
Humans don't want a complete explanation for a prediction but rather compare what the difference were to another instance's prediction (could also be an artificial one).
Making explanations contrastive is application dependent, because it requires a point of reference for comparison.
And this might depend on the data point to be explained, but also on the user receiving an explanation.
A user of a house price prediction website might want to have an explanation of a house price prediction contrastive to her own house or maybe to some other house on the website or maybe to an average house in the neighbourhood.
The solution for creating contrastive explanations in an automated fashion might include finding prototypes or archetypes in the data to contrast to.

**Explanations are selected**:
People don't expect explanations to cover the actual and complete list of causes of an event.
We are used to selecting one or two causes from a huge number of possible causes as THE explanation.
For proof, switch on the television and watch some news:
"The drop in share prices is blamed on a growing backlash against the product due to problems consumers are reporting with the latest software update.",
"Tsubasa and his team lost the match because of a weak defence: they left their opponents to much free space to play out their strategy.",
"The increased distrust in established institutions and our government are the main factors that reduced voter turnout."
The fact that an event can be explained by different causes is called the Rashomon Effect.
Rashomon is a Japanese movie in which alternative, contradictory stories (explanations) of a samurai's death are told.
For machine learning models it is beneficial, when a good prediction can be made from different features.
Ensemble methods can combine multiple models with different features (different explanations) and thrive because averaging over those "stories" makes the predictions more robust and accurate.
But it also means that there is no good selective explanation why they made the prediction.
**What it means for interpretable machine learning**:
Make the explanation very short, give only 1 to 3 reasons, even if the world is more complex.
The [LIME method](#lime) does a good job with this.


**Explanations are social**:
They are part of a conversation or interaction between the explainer and the receiver of the explanation.
The social context determines the content and type of explanations.
If I wanted to explain why digital cryptocurrencies are worth so much, to a technical person I would say things like:
"The decentralised, distributed blockchain-based ledger that cannot be controlled by a central entity resonates with people's desire to secure their wealth, which explains the high demand and price.".
But to my grandma I might say:
"Look Grandma: Cryptocurrencies are a bit like computer gold. People like and pay a lot for gold, and young people like and pay a lot for computer gold."
**What it means for interpretable machine learning**:
Be mindful of the social setting of your machine learning application and of the target audience.
Getting the social part of the machine learning model right depends completely on your specific application.
Find experts from the humanities (e.g. psychologists and sociologists) to help you out.

**Explanations focus on the abnormal**.
People focus more on abnormal causes to explain events (Kahnemann 1981[^Kahnemann]).
These are causes, that had a small likelihood but happened anyways (counterfactual explanation).
And removing these abnormal causes would have changed the outcome a lot.
Humans consider these kinds of "abnormal" causes to be
 good explanations.
An example (Štrumbelj and Kononenko (2011)[^Strumbelj2011]):
Assume that we have a dataset of test situations between teachers and students.
The teachers have the option to directly let students pass a course after they have given a presentation or they can ask additional questions to test the student's knowledge, which determines if the student passes.
This means we have one feature 'teacher'-feature, which is either 0 (teacher does not test) or 1 (teacher does test).
The students can have different levels of preparation (student feature), which translate to different probabilities of correctly answering the teacher's question (in case she decides to test the student).
We want to predict if a student will pass the course and explain our prediction.
The chance to pass is 100% if the teacher does not ask additional questions, else the probability to pass is according to the student's level of preparation and the resulting probability to correctly answer the questions.
Scenario 1:
The teacher asks the students additional questions most of the time (e.g. 95 out of 100 times).
A student who did not study (10% chance to pass the questions part)  was not among the lucky ones and gets additional questions, which he fails to correctly answer.
Why did the student fail the course?
We would say it was the student's fault to not study.
Scenario 2:
The teacher rarely asks additional questions (e.g. 3 out of 100 times).
For a student who did not learn for possible questions, we would still predict a high probability to pass the course, since questions are unlikely.
Of course, one of the students did not prepare for the questions (resulting in a 10% chance to pass the questions).
He is unlucky and the teacher asks additional questions, which the student cannot answer and he fails the course.
What is the reason for failing?
I'd argue that now, the better explanation is that the teacher did test the student, because it was unlikely that the teacher would test.
The teacher feature had an abnormal value.
**What it means for interpretable machine learning**:
If one of the input features for a prediction was abnormal in any sense (like a rare category of a categorical feature) and the feature influenced the prediction, it should be included in an explanation, even if other 'normal' features have the same influence on the prediction as the abnormal one.
An abnormal feature in our house price predictor example might be that a rather expensive house has three balconies.
Even if some attribution method finds out that the three balconies contribute the same price difference as the above average house size, the good neighbourhood and the recent renovation, the abnormal feature "three balconies" might be the best explanation why the house is so expensive.

**Explanations are truthful**.
Good explanations prove to be true in reality (i.e. in other situations).
But, disturbingly, this is not the most important factor for a 'good' explanation.
For example selectiveness is more important than truthfulness.
An explanation that selects only one or two possible causes can never cover the complete list of causes.
Selectivity omits part of the truth.
It's not true that only one or two factors caused a stock market crash for example, but the truth is that there are millions of causes that influence millions of people to act in a way that caused a crash in the end.
**What it means for interpretable machine learning**:
The explanation should predict the event as truthfully as possible, which is sometimes called **fidelity** in the context of machine learning.
So when we say that three balconies increase the price of a house, it should hold true for other houses as well (or at least for similar houses).
To humans, fidelity is not as important for a good explanations as selectivity, contrast and the social aspect.

**Good explanations are coherent with prior beliefs of the explainee**.
Humans tend to ignore information that is not coherent with their prior beliefs.
This effect is known as confirmation bias (Nickerson 1998[^Nickerson]).
Explanations are not spared from this type of bias:
People will tend to devalue or ignore explanations that do not cohere with their beliefs.
This of course differs individually, but there are also group-based prior beliefs like political opinions.
**What it means for interpretable machine learning**:
Good explanations are consistent with prior beliefs.
This one is hard to infuse into machine learning and would probably drastically compromise predictive accuracy.
An example would be negative effects of house size for the predicted price of the house for a few of the houses, which, let's assume, improves accuracy (because of some complex interactions), but strongly contradicts prior beliefs.
One thing you can do is to enforce monotonicity constraints (a feature can affect the outcome only into one direction) or use something like a linear model that has this property.

**Good explanations are general and probable**.
A cause that can explain a lot of events is very general and could be considered as a good explanation.
Note that this contradicts the fact that people explain things with abnormal causes.
As I see it, abnormal causes beat general causes.
Abnormal causes are, by definition, rare.
So in the absence of some abnormal event, a general explanation is judged to be good by humans.
Also keep in mind that people tend to judge probabilities of joint events incorrectly.
(Joe is a librarian. Is it more likely that he is shy or that he is a shy person that loves reading books?).
A good example is 'The bigger a house the more expensive it is', which is a very general, good explanation why houses are expensive or cheap.
**What it means for interpretable machine learning**:
Generality is easily measured by a feature's support, which is the number of instances for which the explanation applies over the total number of instances.

[^Miller2017]: Miller, Tim. 2017. "Explanation in Artificial Intelligence: Insights from the Social Sciences." arXiv Preprint arXiv:1706.07269.

[^Doshi2017]: Doshi-Velez, Finale, and Been Kim. 2017. "Towards A Rigorous Science of Interpretable Machine Learning," no. Ml: 1–13. http://arxiv.org/abs/1702.08608.

[^Heider]: Heider, Fritz, and Marianne Simmel. 1944. "An Experimental Study of Apparent Behavior." The American Journal of Psychology 57 (2). JSTOR: 243–59.

[^Lipton2016]: Lipton, Zachary C. 2016. "The Mythos of Model Interpretability." ICML Workshop on Human Interpretability in Machine Learning, no. Whi.

[^Kahnemann]: Kahneman, Daniel, and Amos Tversky. 1981. "The Simulation Heuristic." STANFORD UNIV CA DEPT OF PSYCHOLOGY.

[^Strumbelj2011]: Štrumbelj, Erik, and Igor Kononenko. 2011. "A General Method for Visualizing and Explaining Black-Box Regression Models." In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer.

[^Nickerson]: Nickerson, Raymond S. 1998. "Confirmation Bias: A Ubiquitous Phenomenon in Many Guises." Review of General Psychology 2 (2). Educational Publishing Foundation: 175.
